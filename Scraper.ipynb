{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем все мемы интернета\n",
    "![catch-em-all](https://static.giantbomb.com/uploads/original/0/1481/2897229-9939923796-latest)\n",
    "Нам предстоит соскрейпить всю интересную информацию с [**knowyourmeme.com**](https://knowyourmeme.com).  \n",
    "\n",
    "Решение задачи, на данный момент, можно разбить на две части:  \n",
    "1.  Получение ссылок со страниц.  \n",
    "Сайт загружает по 16 мемов на страницу, необходимо вытащить на них ссылки, а также реализовать какой-нибудь механизм загрузки других страниц.\n",
    "\n",
    "2.  Получение информации со страницы мема  \n",
    "Попробуем спарсить всю статистику по мему, а также ссылку на картинку и его текстовое описание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Получение ссылок со страницы\n",
    " ### 1.1. Первый запрос\n",
    "  Подключим библиотеку Requests. Она позволяет выполнять HTTP-запросы и обладает очень простым синтаксисом. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем произвести наш первый запрос к серверу. В Requests GET-запрос выполняется следующим образом. Чтобы убедиться, что все прошло хорошо, выведем статус-код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://knowyourmeme.com/memes'\n",
    "request = requests.get(url)\n",
    "request.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, вместо желанного 200 (OK) мы получили 403 (Forbidden). Связано это с тем, что сервер посмотрел на наш User Agent и распознал в нас бота. Для того, чтобы обойти это, импортнем ```UserAgent``` из библиотеки ```fake_useragent```. Затем создадим словарь, в котором сгенерим *человеческого* юзер-агента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ua = UserAgent()\n",
    "header = {'User-Agent':str(ua.chrome)}\n",
    "request = requests.get(url, headers=header)\n",
    "request.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, на этот раз все прошло успешно. Впредь будем использовать этот словарь каждый раз, когда будем делать запрос к серверу.\n",
    "\n",
    "## 1.2. Парсинг HTML с помощью Beautiful Soup  \n",
    "Успешно выполнив запрос с помощью ```requests``` мы получаем допуступ к html-коду страницы. Писать свой парсер мы, конечно, не будем, посколько все уже изобретено до нас. Например, библиотека Beautiful Soup. Она позволяет искать нужные тэги и извлекать из них информацию. То, что нам  нужно.  \n",
    "Импортируем ```BeautifulSoup``` из ```bs4```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы начать что-то искать на странице, нужно создать экземпляр класса BeautifulSoup. Итак, *сварим суп*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(request.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Его содержимое будет выглядеть таким образом:\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html xmlns=\"https://www.w3.org/1999/xhtml\" xmlns:fb=\"https://www.facebook.com/2008/fbml\">\n",
    "<head>\n",
    "<meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
    "<script type=\"text/javascript\">window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"bam.nr-data.net\",\"errorBeacon\":\"bam.nr-data.net\",\"licenseKey\":\"c1a6d52f38\",\"applicationID\":\"31165848\",\"transactionName\":\"dFdfRUpeWglTQB8GDUNKWFRLHl8LUldI\",\"queueTime\":0,\"applicationTime\":63,\"agent\":\"\"}</script>\n",
    "...\n",
    "<html/>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем найти первый тэг ```<a>```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/\" title=\"Know Your Meme\"></a>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того, чтобы получить содержимое тэга тоже ничего придумывать не надо. Достаточно обратиться к полю text. В этом случае поле пустое."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, что есть \"полезного\" в этом тэге - содержимое его аттрибута ```title```. Его можно получить, вызвав ```get(attr)```, где ```attr``` - название аттрибута."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Know Your Meme'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('a').get('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применим полученные знания для того, чтобы получить ссылки на мемы.  \n",
    "Что нам нужно?\n",
    "* Найти все теги ```<a>```, ведущие на страницы мемов\n",
    "* Вытащить ссылку из аттрибута ```href```.  \n",
    "Изучив код страницы с помощью браузера, выясним, что все тэги ```<a>```, которые ведут на желанные мемы, имеют аттрибут ```class = \"photo\"```.\n",
    "\n",
    "Переберем все тэги ```<a>``` на странице и, если они имеют нужный аттрибут, сохраним ссылку в список.\n",
    "\n",
    "Конечно, этот код можно было уместить в одну строку генератором, но оставим \"развернутый\" код."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://knowyourmeme.com/memes/learn-to-code\n",
      "http://knowyourmeme.com/memes/mcdonalds-alignment-chart\n",
      "http://knowyourmeme.com/memes/old-town-road\n",
      "http://knowyourmeme.com/memes/surprised-pikachu\n",
      "http://knowyourmeme.com/memes/cursed-emojis\n",
      "http://knowyourmeme.com/memes/folgers-brother-and-sister-commercial\n",
      "http://knowyourmeme.com/memes/blini-cat\n",
      "http://knowyourmeme.com/memes/subcultures/petscop\n",
      "http://knowyourmeme.com/memes/short-tyler1\n",
      "http://knowyourmeme.com/memes/epic-handshake\n",
      "http://knowyourmeme.com/memes/ok-boomer\n",
      "http://knowyourmeme.com/memes/double-ds-facts-book\n",
      "http://knowyourmeme.com/memes/dont-dead-open-inside\n",
      "http://knowyourmeme.com/memes/fat-yoshi\n",
      "http://knowyourmeme.com/memes/marisaface-kanmarisa\n",
      "http://knowyourmeme.com/memes/woman-yelling-at-a-cat\n"
     ]
    }
   ],
   "source": [
    "url = 'http://knowyourmeme.com'\n",
    "memes_links = []\n",
    "for a in soup.find_all('a'):\n",
    "    if a.get('class') == ['photo']:\n",
    "        link = url + a.get('href')\n",
    "        memes_links.append(link)\n",
    "        \n",
    "for link in memes_links:\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично! Большая часть работы выполнена. Осталось понять, как скрипт будет перемещаться от страницы к странице.\n",
    "## 1.3. Перемещение между страницами  \n",
    "Нетрудно заметить, что при скроллинге каждые 16 мемов подгружается новая страница с мемами и меняется url. Получается что, чтобы получить мемы с опредленной страницы, нужно просто указать ее номер после ```http://knowyourmeme.com/memes/page/```. Проверим нашу *теорию*, попробовав загрузить мемы с первых трех страниц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Page 1:\n",
      "Found 16 memes\n",
      "\n",
      "Page 2:\n",
      "Found 16 memes\n",
      "\n",
      "Page 3:\n",
      "Found 16 memes\n"
     ]
    }
   ],
   "source": [
    "page_template = 'http://knowyourmeme.com/memes/page/{}'\n",
    "for page in range(1,4):\n",
    "    # Formatting page url\n",
    "    page_url = page_template.format(page)\n",
    "    # Creating request\n",
    "    r = requests.get(page_url, headers=header)\n",
    "    # Brewing soup\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    # Using generator to fetch all the links \n",
    "    memes_links = [url + a.get('href') for a in soup.find_all('a') if a.get('class') == ['photo']]\n",
    "    print('\\nPage {}:'.format(page))\n",
    "    print('Found {} memes'.format(len(memes_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждой странице найдено по 16 мемов, как и ожидалось, а значит, все работает!\n",
    "\n",
    "Напишем функцию, будет принимать номер страницы и словарь, содержащий User Agent, и будет возвращать список ссылок на данной странице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(page, header):\n",
    "    '''\n",
    "    Returns list of links to the memes on the page\n",
    "        page: int\n",
    "            number of the page for parsing\n",
    "        header: dict\n",
    "            headers distionary that will be passed in requests.get()\n",
    "        memes_links: [str]\n",
    "            list of links\n",
    "    '''\n",
    "    link = 'http://knowyourmeme.com/memes/all/page/{}'.format(page)\n",
    "    # Sending GET request\n",
    "    try:\n",
    "        response = requests.get(link, headers=header)\n",
    "    except:\n",
    "        return []    \n",
    "    if not response.ok:\n",
    "        print('Error code in get_links:', response.status_code)\n",
    "        return []\n",
    "    # Brewing soup\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    # Lambda to filter all <a> tags that have 'class' attribute set to 'photo'\n",
    "    a_photo_filter = lambda tag: tag.name == 'a' and tag.get('class') == ['photo']\n",
    "    # Creating list of links to the memes\n",
    "    memes_links = ['http://knowyourmeme.com' + a.get('href') for a in soup.find_all(a_photo_filter)]\n",
    "    return memes_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Получение информации со страницы  \n",
    "Итак, мы добрались до самого сложного и интересного.\n",
    "\n",
    "## 2.1. Получение статистики  \n",
    "На каждой (ну или почти на каждой - все может быть...) странице с мемом есть блок, содержащий его статистику с сайта. В нее входит количество просмотров, комментариев, а также загруженных фото и видео.  \n",
    "![stats](https://sun9-17.userapi.com/c205824/v205824456/18eb5/aNFSVt_yZDc.jpg)\n",
    "Вытащить эти данные можно из тега ```<dd>``` с аттрибутом ```class``` с одним из следующих значений ```views, videos, photos, comments```. Аттрибут ```title``` будет хранить необходимую информацию в строке. Однако, в ней будут присуствовать буквы, а также большие числа будут разделены запятой.  \n",
    "Хорошо бы это почистить, верно? Преобразуем строку в нижний регистр, и удалим все запятые. Затем разделим строку на два слова: первое - значение, второе - измеряемая статистика. Запишем полученную пару значение-ключ в словарь.  \n",
    "\n",
    "Применим все это на примере мема с кричащей девушкой и котом.\n",
    "![woman-yelling-at-a-cat](https://i.kym-cdn.com/entries/icons/original/000/030/157/womanyellingcat.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'views': 3978969, 'videos': 22, 'images': 382, 'comments': 34}\n"
     ]
    }
   ],
   "source": [
    "link = 'https://knowyourmeme.com/memes/woman-yelling-at-a-cat'\n",
    "r = requests.get(link, headers=header)\n",
    "soup = BeautifulSoup(r.text)\n",
    "stats_dict = {}\n",
    "# Stats we're searching for\n",
    "stats = ['views', 'videos', 'photos', 'comments']\n",
    "for stat in stats:\n",
    "    dd = soup.find('dd', attrs={'class':stat})\n",
    "    # Some stats maybe not available on some pages,\n",
    "    # so we need to protect ourselves from getting an error\n",
    "    if dd:\n",
    "        val_key_str = dd.get('title').lower().replace(',', '')\n",
    "        value, key = val_key_str.split(' ')\n",
    "        stats_dict[key] = int(value)\n",
    "print(stats_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все работает! Обернем все это в функцию для дальнейшего использования. Будем передавать в функцию уже готовый суп."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(soup, stat):\n",
    "    '''\n",
    "    Return requested stat of a meme\n",
    "        soup: bs4 soup\n",
    "            soup of a meme\n",
    "        stats: string\n",
    "            stat name string: views/videos/photos/comments\n",
    "        value: int\n",
    "            stat of a meme\n",
    "    '''\n",
    "    dd = soup.find('dd', attrs={'class':stat})\n",
    "    value = dd.find_next('a').text.replace(',', '') if dd else 0\n",
    "    return int(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Получение основной информации  \n",
    "Здесь я хочу вытащить название мема, его статус, происхождение, дату появления, тэги, дату добавления на сайт и дату последнего изменения.  \n",
    "\n",
    "Вытащить название максимально просто - это первый тэг ```<h1>``` на странице. Найдем его и получим название с помощью ```.text```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woman Yelling at a Cat\n"
     ]
    }
   ],
   "source": [
    "m_name = soup.find('h1')\n",
    "m_name = m_name.text.strip() if m_name else ''\n",
    "print(m_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вся информация по статусу мема, его происхождению и тэгам вынесена в колонку справа от текста с его историей. Она представляет собой тэг ```<aside>``` с аттрибутом ```class=\"left\"```. Beautiful Soup позволяет вызывать те же функции для поиска к тэгам, найденным в супе. Мы можем использовать это и начать искать внутри ```<aside>```.  \n",
    "![properties](https://sun9-8.userapi.com/c854124/v854124456/1a4429/BMzSXQfMfGA.jpg)\n",
    "Первым тэгом ```<dl>``` внутри будет тэг, содержащий категорию страницы. Запишем его содержимое в отдельную переменную.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meme\n"
     ]
    }
   ],
   "source": [
    "properties = soup.find('aside', attrs={'class':'left'})\n",
    "m_category = properties.find('dl')\n",
    "m_category = m_category.find_next()\n",
    "m_category = m_category.text.strip() if m_category else ''\n",
    "print(m_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статус мема можно найти в первом тэге ```<dd>```.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed\n"
     ]
    }
   ],
   "source": [
    "m_status = properties.find('dd')\n",
    "m_status = m_status.text.strip() if m_status else ''\n",
    "print(m_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тип мема получим, найдя первый тэг ```<a class=\"entry-type-link\" ...><a/>```.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploitable\n"
     ]
    }
   ],
   "source": [
    "m_type = properties.find('a', attrs={'class':'entry-type-link'})\n",
    "m_type = m_type.text.strip() if m_type else ''\n",
    "print(m_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup также позволяет искать тэги по тексту, который они содержат. Найдем первый тэг, содержащий в тексте ```\\nYear\\n```, а затем после него найдем следующую ссылку. В тексте этой ссылки будет год появления мема.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019\n"
     ]
    }
   ],
   "source": [
    "m_year = properties.find(text='\\nYear\\n')\n",
    "m_year = m_year.find_next('a') if m_year else ''\n",
    "m_year = int(m_year.text.strip()) if m_year else ''\n",
    "print(m_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично тому, как мы нашли статус мема, найдем ```<dd>``` с классом ```class=\"entry_origin_link\"```. Следующей ссылкой будет ссылка с текстом, обозначающим то, откуда пошел мем.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter\n"
     ]
    }
   ],
   "source": [
    "m_origin = properties.find('dd', attrs={'entry_origin_link'})\n",
    "m_origin = m_origin.find_next('a') if m_origin else ''\n",
    "m_origin = m_origin.text.strip() if m_origin else ''\n",
    "print(m_origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем тэги, которые добавлены к мему, таким же образом, как искали год зарождения мема - по содержимому тэга.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat, food, dinner, chair, the real housewives of beverly hills, taylor armstrong, kyle richards, deadbefordeath, perpetualwinter, 69-, apple-trump, malibu beach party from hell, missingegirl, lc28__, salad cat, table cat, lady screaming at cat, smudge, ku\n"
     ]
    }
   ],
   "source": [
    "m_tags = properties.find(text='\\nTags\\n')\n",
    "m_tags = m_tags.find_next() if m_tags else ''\n",
    "m_tags = m_tags.text.strip() if m_tags else ''\n",
    "print(m_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вытащим дату из тэга ```abbr``` с классом ```class=\"timeago\"```. Однако, среди найденных тэгов нашлось и что-то лишнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = soup.find_all('abbr', attrs={'class':'timeago'})\n",
    "len(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужные нам тэги будут содержать строки ```\"Added\"``` или ```\"Updated\"```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-20T11:14:05-04:00\n",
      "2019-11-24T02:06:55-05:00\n"
     ]
    }
   ],
   "source": [
    "for t in times:\n",
    "    # Split parent's tag text\n",
    "    time = t.parent.text.split('\\n')\n",
    "    # If added or updated found in list, get\n",
    "    if 'Added' in time:\n",
    "        added = t.get('title')\n",
    "    elif 'Updated' in time:\n",
    "        updated = t.get('title')\n",
    "\n",
    "print(added)\n",
    "print(updated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет возвращать словарь из полученных значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_properties(soup):\n",
    "    '''\n",
    "    Return dictionary containing properties of a meme\n",
    "        soup: bs4 soup\n",
    "            soup of a meme\n",
    "        properties_dict: {str:[str/int]}\n",
    "            dict of all useful properties\n",
    "    '''\n",
    "    # Creating return dict\n",
    "    properties_dict = {}\n",
    "\n",
    "    # Name\n",
    "    m_name = soup.find('h1')\n",
    "    m_name = m_name.text.strip() if m_name else ''\n",
    "    properties_dict['name'] = m_name\n",
    "\n",
    "    # Finding <aside> tag contaning needed information\n",
    "    properties = soup.find('aside', attrs={'class':'left'})\n",
    "    if properties:\n",
    "        # Category\n",
    "        m_category = properties.find('dl')\n",
    "        m_category = m_category.find_next()\n",
    "        m_category = m_category.text.strip() if m_category else ''\n",
    "        properties_dict['category'] = m_category.lower()\n",
    "\n",
    "        m_status = properties.find_next('dd')\n",
    "        m_status = m_status.text.strip() if m_status else ''\n",
    "        properties_dict['status'] = m_status.lower()\n",
    "\n",
    "        m_type = properties.find('a', attrs={'class':'entry-type-link'})\n",
    "        m_type = m_type.text.strip() if m_type else ''\n",
    "        properties_dict['type'] = m_type.lower()\n",
    "\n",
    "        m_year = properties.find(text='\\nYear\\n')\n",
    "        m_year = m_year.find_next() if m_year else ''\n",
    "        if m_year:\n",
    "            m_year = int(m_year.text.strip()) if m_year.text.strip().isdigit() else m_year.text.strip().lower()\n",
    "        properties_dict['year'] = m_year\n",
    "\n",
    "        m_origin = properties.find('dd', attrs={'entry_origin_link'})\n",
    "        m_origin = m_origin.text.strip() if m_origin else ''\n",
    "        properties_dict['origin'] = m_origin.lower()\n",
    "\n",
    "        m_tags = properties.find(text='\\nTags\\n')\n",
    "        m_tags = m_tags.find_next() if m_tags else ''\n",
    "        m_tags = m_tags.text.strip() if m_tags else ''\n",
    "        properties_dict['tags'] = m_tags.lower()\n",
    "\n",
    "    # Fetching dates\n",
    "    times = soup.find_all('abbr', attrs={'class':'timeago'})\n",
    "    for t in times:\n",
    "        # Split parent's tag text\n",
    "        time = t.parent.text.split('\\n')\n",
    "        # If added or updated found in list, get\n",
    "        if 'Added' in time:\n",
    "            properties_dict['added'] = t.get('title')\n",
    "        elif 'Updated' in time:\n",
    "            properties_dict['updated'] = t.get('title')\n",
    "    return properties_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Получение текстового описания  \n",
    "Весь текст находится в тэге ```<section>``` с тэгом ```class=\"bodycopy\"```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bodycopy = soup.find('section', attrs={'class':'bodycopy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Краткое описание мема из секции About находится в первом тэге ```<p>```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Woman Yelling at a Cat refers to a meme format featuring a screen cap of The Real Housewives of Beverly Hills cast members Taylor Armstrong and Kyle Richards followed by a picture of a confused-looking cat sitting behind a dinner plate. The format gained significant popularity across the web in mid-June 2019 and the cat was later identified as Smudge the Cat.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "about = bodycopy.find('p')\n",
    "about.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В секции Origin (где-то ее название - History) описано происхождение мема. Найдем первый тэг, в тексте которого есть эти слова. Следующий тэг ```<p>``` будет содержать текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On December 5th, 2011, episode 14 \"Malibu Beach Party From Hell\" of season two of The Real Housewives of Beverly Hills reality TV series premiered in the United States.[1] In the episode, cast member Taylor Armstrong cries during an argument, with cast member Kyle Richards attempting to calm her down. (shown below).'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_h = bodycopy.find(text='Origin') or bodycopy.find(text='History')\n",
    "next_h.find_next('p').text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, не весь текст находится в одном параграфе. Связано это тем, что практически на каждой странице вперемешку с картинками. Будем двигаться по тегам внутри цикла, пока не наткнемся на тэг ```<h2>```, что будет означать окончание этой секции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On December 5th, 2011, episode 14 \"Malibu Beach Party From Hell\" of season two of The Real Housewives of Beverly Hills reality TV series premiered in the United States.[1] In the episode, cast member '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = ''\n",
    "while next_h and next_h.name != 'h2':\n",
    "    if next_h.name == 'p':\n",
    "        history += next_h.text.strip()\n",
    "    next_h = next_h.find_next()\n",
    "history[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Весь остальной текст вытащим похожим образом. Однако, тут могут встретиться и другие заголовки, поэтому в этот раз остановимся когда дойдем до заголовка с названием Search Interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On May 2nd, Twitter user @lc28__ made the first known meme based on the format, gaining over 20 retweets and 180 likes (shown below, left).[7] On June 2nd, 2019, Redditor PerpetualWinter made the firs'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_o = next_h.find_next('p') if next_h else ''\n",
    "other = ''\n",
    "while next_o and (next_o.name != 'h2' and next_o.text != 'Search Interest'):\n",
    "    if next_o.name == 'p':\n",
    "        other += next_o.text.strip()\n",
    "    next_o = next_o.find_next()\n",
    "other[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тексте часто встречаются ссылки в квадратных скобках, а также сноски в круглых. Поскольку нам этого не нужно, напишем функцию, которая будет удалять это удалять. Сделаем ее вложенной в фукнции, которая будет возвращать словарь с текстовой информацией. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(soup):\n",
    "    '''\n",
    "    Returns dictionary of text info\n",
    "        soup: bs4 soup\n",
    "            soup of a meme\n",
    "        text_dict: {str:str}\n",
    "            dictionary of text info\n",
    "    '''\n",
    "    def remove_brackets(string, brackets):\n",
    "        opening, closing = brackets\n",
    "        # If deleting brackets\n",
    "        if brackets == '[]':\n",
    "            start = string.find(opening)\n",
    "            end = string.find(closing)\n",
    "            while start != -1 and end != -1:\n",
    "                string = string.replace(string[start:end+1], '')\n",
    "                start = string.find(opening)\n",
    "                end = string.find(closing)\n",
    "        # Else, deleting '(shown below)'\n",
    "        else:\n",
    "            substring = '(shown below)'\n",
    "            index = string.find(substring)\n",
    "            while index != -1:\n",
    "                string = string.replace(substring, '')\n",
    "                index = string.find(substring)\n",
    "        return string\n",
    "\n",
    "    text_dict = {}\n",
    "    bodycopy = soup.find('section', attrs={'class':'bodycopy'})\n",
    "    if bodycopy:\n",
    "        # About text\n",
    "        about = bodycopy.find('p')\n",
    "        about = about.text.strip() if about else ''\n",
    "        about = remove_brackets(about, '()')\n",
    "        #about = remove_brackets(about, '[]')\n",
    "        text_dict['about'] = about\n",
    "\n",
    "        # Origin or history\n",
    "        next_h = bodycopy.find(text='Origin') or bodycopy.find(text='History')\n",
    "        next_h = next_h.find_next('p') if next_h else ''\n",
    "        history = ''\n",
    "        while next_h and next_h.name != 'h2':\n",
    "            if next_h.name == 'p':\n",
    "                history += next_h.text.strip()\n",
    "            next_h = next_h.find_next()\n",
    "        history = remove_brackets(history, '()')\n",
    "        #history = remove_brackets(history, '[]')\n",
    "        text_dict['history'] = history\n",
    "\n",
    "        # Every other text\n",
    "        next_o = next_h.find_next('p') if next_h else ''\n",
    "        other = ''\n",
    "        while next_o and (next_o.name != 'h2' and next_o.text != 'Search Interest'):\n",
    "            if next_o.name == 'p':\n",
    "                other += next_o.text.strip()\n",
    "            next_o = next_o.find_next()\n",
    "        other = remove_brackets(other, '()')\n",
    "        #other = remove_brackets(other, '[]')\n",
    "        text_dict['other'] = other\n",
    "\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Получение ссылки на картинку\n",
    "Сразу качать мемы мы не будем, поэтому просто вытащим ссылку на картинку.  \n",
    "Мы будем брать картинку, которая является превью мема. Она находится в тэге ```<a>``` с аттрибутом ```class=\"photo left wide\"``` или ```class=\"photo left\"``` для более старых мемов. Сначала попоробуем найти картинку с первым аттрибутом, и потом, если ничего не нашли, попробуем найти по второму аттрибуту.  \n",
    "Однако, с тэгом ```class=\"photo left\"``` есть еще и картинки ленты вверху сайта, в которой показаны последние добавленные мемы. Все они ведут на страницу мема и имеют адрес ```/memes/...``` в тэге ```href```, поэтому их будет легко отфильтровать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://i.kym-cdn.com/entries/icons/original/000/030/157/womanyellingcat.jpg\n"
     ]
    }
   ],
   "source": [
    "a = soup.find('a', attrs={'class':'photo left wide'})\n",
    "if not a:\n",
    "    for a in soup.find_all('a', attrs={'class':'photo left'}):\n",
    "        if a.get('href')[:6] != '/memes':\n",
    "            break\n",
    "link = a.get('href') if a else ''\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже почти по традиции, напишем отдельную функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pic_link(soup):\n",
    "    '''\n",
    "        soup: bs4 soup\n",
    "            soup of a meme page\n",
    "        link: str\n",
    "            link to a pic\n",
    "    '''\n",
    "    a = soup.find('a', attrs={'class':'photo left wide'})\n",
    "    if not a:\n",
    "        for a in soup.find_all('a', attrs={'class':'photo left'}):\n",
    "            if a.get('href')[:6] != '/memes':\n",
    "                break\n",
    "    link = a.get('href') if a else ''\n",
    "    return link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Объединение всего в одну функцию\n",
    "Напишем функцию, которая будет вызывать написанные ранее функции. Было бы удобно скрыть отправку запроса и варку супа внутрь самой функции, чтобы аргументамии были бы ссылка на мем и хедер для ```requests```.  \n",
    "Возможно, стоит объяснить, зачем подавать в функцию хедер. Дело в том, что если каждый раз генерировать нового юзер-агента, для сервера это тоже станет подозрительно и мы опять получим 403, будто мы и вовсе не подменяли User Agent. И правда, вряд ли реальный человек каждый запрос будет отправлять с нового браузера. Поэтому необходимо сгенерировать юзер-агент один раз, а затем использовать его для каждого запроса.\n",
    "\n",
    "Также подключим ```datetime```, чтобы сохранять момент, когда был отпарсена страница."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(link, header):\n",
    "    '''\n",
    "    Scraps the data from link\n",
    "        link: str\n",
    "            link to the meme\n",
    "        data: {str:str/int}\n",
    "            Dict contaning row of dataframe\n",
    "    '''\n",
    "    try:\n",
    "        response = requests.get(link, headers=header)\n",
    "    except:\n",
    "        return {}\n",
    "    if not response.ok:\n",
    "        print(\"Error code in get_data:\", response.status_code)\n",
    "        return {}\n",
    "    soup = BeautifulSoup(response.text, features='lxml')\n",
    "    # Creating return dict\n",
    "    data = {}\n",
    "    for stat in ['views', 'videos', 'photos', 'comments']:\n",
    "        data[stat] = get_stats(soup, stat)\n",
    "    # Updating with dicts returned by previously written functions\n",
    "    data.update(get_properties(soup))\n",
    "    data.update(get_text(soup))\n",
    "    # Adding pic link\n",
    "    data['picture'] = get_pic_link(soup)\n",
    "    data['scraped_at'] = datetime.now()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключим ```pandas``` и посмотрим на то, чего мы достигли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Woman Yelling at a Cat'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_data('http://knowyourmeme.com/memes/woman-yelling-at-a-cat', header)\n",
    "data['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>status</th>\n",
       "      <th>year</th>\n",
       "      <th>added</th>\n",
       "      <th>updated</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>type</th>\n",
       "      <th>about</th>\n",
       "      <th>history</th>\n",
       "      <th>other</th>\n",
       "      <th>origin</th>\n",
       "      <th>picture</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woman Yelling at a Cat</td>\n",
       "      <td>meme</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-20T11:14:05-04:00</td>\n",
       "      <td>2019-11-24T02:06:55-05:00</td>\n",
       "      <td>3978969</td>\n",
       "      <td>22</td>\n",
       "      <td>382</td>\n",
       "      <td>34</td>\n",
       "      <td>cat, food, dinner, chair, the real housewives ...</td>\n",
       "      <td>exploitable</td>\n",
       "      <td>Woman Yelling at a Cat refers to a meme format...</td>\n",
       "      <td>On December 5th, 2011, episode 14 \"Malibu Beac...</td>\n",
       "      <td>On May 2nd, Twitter user @lc28__ made the firs...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>https://i.kym-cdn.com/entries/icons/original/0...</td>\n",
       "      <td>2020-01-03 04:20:04.033661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name category     status  year  \\\n",
       "0  Woman Yelling at a Cat     meme  confirmed  2019   \n",
       "\n",
       "                       added                    updated    views videos  \\\n",
       "0  2019-06-20T11:14:05-04:00  2019-11-24T02:06:55-05:00  3978969     22   \n",
       "\n",
       "  photos comments                                               tags  \\\n",
       "0    382       34  cat, food, dinner, chair, the real housewives ...   \n",
       "\n",
       "          type                                              about  \\\n",
       "0  exploitable  Woman Yelling at a Cat refers to a meme format...   \n",
       "\n",
       "                                             history  \\\n",
       "0  On December 5th, 2011, episode 14 \"Malibu Beac...   \n",
       "\n",
       "                                               other   origin  \\\n",
       "0  On May 2nd, Twitter user @lc28__ made the firs...  twitter   \n",
       "\n",
       "                                             picture  \\\n",
       "0  https://i.kym-cdn.com/entries/icons/original/0...   \n",
       "\n",
       "                  scraped_at  \n",
       "0 2020-01-03 04:20:04.033661  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['name', 'category', 'status','year', 'added', 'updated',\n",
    "                       'views', 'videos', 'photos', 'comments', 'tags', 'type',\n",
    "                       'about', 'history', 'other', 'origin', 'picture', 'scraped_at'])\n",
    "df = df.append(data, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Обход блокировок с помощью Tor\n",
    "Казалось бы, все готово - можно написать какой-то простенький цикл и запускать скрипт. Однако, рано или поздно, мы превысим количество запросов и сервер нас забанит.  \n",
    "Попробуем обойти это, начав скрейпить через Tor. На каждую ошибку будем менять IP и генерировать новый User Agent.  \n",
    "Для работы с Tor подключим библиотеку ```stem```, а вернее импортнем оттуда классы ```Controller``` и ```Signal```, а для того, чтобы использовать открытый Tor как прокси, подлючим модули ```socket``` и ```socks```. Модуль ```time``` нужен для того, чтобы вызывать ```sleep``` на время, необходимое тору, чтобы сменить ip. Теперь запустим Tor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stem.control import Controller\n",
    "from stem import Signal\n",
    "import socket\n",
    "import socks\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "socks.set_default_proxy(socks.SOCKS5, \"localhost\", 9150)\n",
    "socket.socket = socks.socksocket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая будет возвращать наш ip-адрес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_ip():\n",
    "    ip_url = 'https://api.ipify.org'\n",
    "    r = requests.get(ip_url)\n",
    "    if r.ok:\n",
    "        return r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вывести результат функции, получим ip, отличный от нашего настоящего, а значит мы успешно подключились к тору.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.228.45.176\n"
     ]
    }
   ],
   "source": [
    "print(check_ip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, работает ли наша функция ```get_data```. Используем новый хедер со сгенерированным User Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>status</th>\n",
       "      <th>year</th>\n",
       "      <th>added</th>\n",
       "      <th>updated</th>\n",
       "      <th>views</th>\n",
       "      <th>videos</th>\n",
       "      <th>photos</th>\n",
       "      <th>comments</th>\n",
       "      <th>tags</th>\n",
       "      <th>type</th>\n",
       "      <th>about</th>\n",
       "      <th>history</th>\n",
       "      <th>other</th>\n",
       "      <th>origin</th>\n",
       "      <th>picture</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woman Yelling at a Cat</td>\n",
       "      <td>meme</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2019</td>\n",
       "      <td>2019-06-20T11:14:05-04:00</td>\n",
       "      <td>2019-11-24T02:06:55-05:00</td>\n",
       "      <td>3978969</td>\n",
       "      <td>22</td>\n",
       "      <td>382</td>\n",
       "      <td>34</td>\n",
       "      <td>cat, food, dinner, chair, the real housewives ...</td>\n",
       "      <td>exploitable</td>\n",
       "      <td>Woman Yelling at a Cat refers to a meme format...</td>\n",
       "      <td>On December 5th, 2011, episode 14 \"Malibu Beac...</td>\n",
       "      <td>On May 2nd, Twitter user @lc28__ made the firs...</td>\n",
       "      <td>twitter</td>\n",
       "      <td>https://i.kym-cdn.com/entries/icons/original/0...</td>\n",
       "      <td>2020-01-03 04:20:04.033661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Doge</td>\n",
       "      <td>meme</td>\n",
       "      <td>confirmed</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013-07-24T16:29:55-04:00</td>\n",
       "      <td>2019-12-06T08:30:40-05:00</td>\n",
       "      <td>13122848</td>\n",
       "      <td>62</td>\n",
       "      <td>1666</td>\n",
       "      <td>918</td>\n",
       "      <td>animal, dog, shiba inu, shibe, such doge, supe...</td>\n",
       "      <td>animal</td>\n",
       "      <td>Doge is a slang term for \"dog\" that is primari...</td>\n",
       "      <td>The use of the misspelled word \"doge\" to refer...</td>\n",
       "      <td>On October 28th, 2010, a photo of Kabosu was s...</td>\n",
       "      <td>tumblr</td>\n",
       "      <td>https://i.kym-cdn.com/entries/icons/original/0...</td>\n",
       "      <td>2020-01-03 04:22:18.957737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name category     status  year  \\\n",
       "0  Woman Yelling at a Cat     meme  confirmed  2019   \n",
       "1                    Doge     meme  confirmed  2013   \n",
       "\n",
       "                       added                    updated     views videos  \\\n",
       "0  2019-06-20T11:14:05-04:00  2019-11-24T02:06:55-05:00   3978969     22   \n",
       "1  2013-07-24T16:29:55-04:00  2019-12-06T08:30:40-05:00  13122848     62   \n",
       "\n",
       "  photos comments                                               tags  \\\n",
       "0    382       34  cat, food, dinner, chair, the real housewives ...   \n",
       "1   1666      918  animal, dog, shiba inu, shibe, such doge, supe...   \n",
       "\n",
       "          type                                              about  \\\n",
       "0  exploitable  Woman Yelling at a Cat refers to a meme format...   \n",
       "1       animal  Doge is a slang term for \"dog\" that is primari...   \n",
       "\n",
       "                                             history  \\\n",
       "0  On December 5th, 2011, episode 14 \"Malibu Beac...   \n",
       "1  The use of the misspelled word \"doge\" to refer...   \n",
       "\n",
       "                                               other   origin  \\\n",
       "0  On May 2nd, Twitter user @lc28__ made the firs...  twitter   \n",
       "1  On October 28th, 2010, a photo of Kabosu was s...   tumblr   \n",
       "\n",
       "                                             picture  \\\n",
       "0  https://i.kym-cdn.com/entries/icons/original/0...   \n",
       "1  https://i.kym-cdn.com/entries/icons/original/0...   \n",
       "\n",
       "                  scraped_at  \n",
       "0 2020-01-03 04:20:04.033661  \n",
       "1 2020-01-03 04:22:18.957737  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = {'User-Agent':str(ua.chrome)}\n",
    "url = 'https://knowyourmeme.com/memes/doge'\n",
    "data = get_data(url, header)\n",
    "df = df.append(data, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично, все работает! Теперь все это можно собирать в скрипт. Нужно будет написать основной цикл со сменой ip каждую ошибку."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
